{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import List, Tuple, Union, Callable, Dict\n",
    "import torch\n",
    "from functools import partial\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerard/anaconda3/envs/feature-circuits/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "\n",
    "transcoder_template = \"/media/workspace/gpt-2-small-transcoders/final_sparse_autoencoder_gpt2-small_blocks.{}.ln2.hook_normalized_24576\"\n",
    "\n",
    "tcs_dict = {}\n",
    "for i in range(5):\n",
    "    tc = SparseAutoencoder.load_from_pretrained(f\"{transcoder_template.format(i)}.pt\").eval()\n",
    "    tcs_dict[tc.cfg.hook_point] = tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005)\n"
     ]
    }
   ],
   "source": [
    "logits,cache = model.run_with_cache(\"The\")\n",
    "print(cache[\"blocks.3.hook_resid_pre\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_hooks(*hooks):\n",
    "    \"\"\"\n",
    "    Compose multiple hooks into a single hook by executing them in order.\n",
    "    \"\"\"\n",
    "    def composed_hook(tensor: torch.Tensor, hook: HookPoint):\n",
    "        for hook_fn in hooks:\n",
    "            tensor = hook_fn(tensor, hook)\n",
    "        return tensor\n",
    "    return composed_hook\n",
    "\n",
    "def retain_grad_hook(tensor: torch.Tensor, hook: HookPoint):\n",
    "    \"\"\"\n",
    "    Retain the gradient of the tensor at the given hook point.\n",
    "    \"\"\"\n",
    "    tensor.retain_grad()\n",
    "    return tensor\n",
    "\n",
    "def detach_hook(tensor: torch.Tensor, hook: HookPoint):\n",
    "    \"\"\"\n",
    "    Detach the tensor at the given hook point.\n",
    "    \"\"\"\n",
    "    return tensor.detach().requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.ln2.hook_normalized\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.4.ln2.hook_normalized\n",
      "tensor(-0.1963, grad_fn=<SumBackward0>)\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.4.ln2.hook_normalized\n",
      "tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "fwd_hooks: list[Tuple[Union[str, Callable], Callable]] = []\n",
    "bwd_hooks: list[Tuple[Union[str, Callable], Callable]] = []\n",
    "\n",
    "\n",
    "def sae_bwd_hook(grad: torch.Tensor,hook: HookPoint):\n",
    "    print(hook.name)\n",
    "    return (grad,)\n",
    "cache_rec = {}\n",
    "def get_fwd_hooks(sae: SparseAutoencoder) -> list[Tuple[Union[str, Callable], Callable]]:\n",
    "    x = None\n",
    "    def hook_in(tensor: torch.Tensor, hook: HookPoint):\n",
    "        print(hook.name)\n",
    "        nonlocal x\n",
    "        x = tensor\n",
    "        return tensor\n",
    "    def hook_out(tensor: torch.Tensor, hook: HookPoint):\n",
    "        nonlocal x\n",
    "        assert x is not None, \"hook_in must be called before hook_out.\"\n",
    "        reconstructed,_,_,_,_,_, = sae.forward(x)\n",
    "        x = None\n",
    "        cache_rec[hook.name] = reconstructed + (tensor - reconstructed).detach()\n",
    "\n",
    "        return reconstructed + (tensor - reconstructed).detach()\n",
    "    return [(sae.cfg.hook_point, hook_in), (sae.cfg.out_hook_point, hook_out)]\n",
    "for sae in tcs_dict.values():\n",
    "    hooks = get_fwd_hooks(sae)\n",
    "    fwd_hooks.extend(hooks)\n",
    "\n",
    "bwd_hooks=[(val.cfg.out_hook_point, sae_bwd_hook) for val in tcs_dict.values()]\n",
    "model.reset_hooks()\n",
    "for name, sae in tcs_dict.items():\n",
    "    module_name = \"sae\"\n",
    "    hook_point = model.mod_dict[name]\n",
    "    hook_point._modules[\"sae\"] = tcs_dict[name]\n",
    "model.setup()\n",
    "with model.hooks(fwd_hooks, bwd_hooks):\n",
    "    \n",
    "    logits = model(\"The\")\n",
    "    print(logits.sum())\n",
    "    logits,cache = model.run_with_cache(\"Hello world\")\n",
    "\n",
    "    print(cache[\"blocks.3.hook_resid_pre\"].sum())\n",
    "    logits.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005)\n",
      "tensor(-2.1454)\n",
      "tensor(0.0005, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cache[\"blocks.2.hook_mlp_out\"].sum())\n",
    "print(cache[\"blocks.2.ln2.hook_normalized.sae.hook_sae_out\"].sum())\n",
    "print(cache_rec[\"blocks.2.hook_mlp_out\"].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
